{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFLLl1Agum8O"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pinecone-io/examples/blob/master/docs/gpt-4-langchain-docs.ipynb) [![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/nbviewer-shield.svg)](https://nbviewer.org/github/pinecone-io/examples/blob/master/docs/gpt-4-langchain-docs.ipynb)\n",
        "\n",
        "# GPT4 with Retrieval Augmentation over LangChain Docs\n",
        "\n",
        "In this notebook we'll work through an example of using GPT-4 with retrieval augmentation to answer questions about the LangChain Python library.\n",
        "\n",
        "[![Open nbviewer](https://raw.githubusercontent.com/pinecone-io/examples/master/assets/full-link.svg)](https://github.com/pinecone-io/examples/blob/master/learn/generation/openai/gpt-4-langchain-docs.ipynb)\n",
        "\n",
        "To begin we must install the prerequisite libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_HDKlQO5svqI",
        "outputId": "1baa3186-e316-4111-931f-3517e1023632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.4/567.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "    openai==1.66.3 \\\n",
        "    pinecone==5.4.2 \\\n",
        "    pinecone-datasets==1.0.2 \\\n",
        "    pinecone-notebooks==0.1.1 \\\n",
        "    tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from pinecone_datasets import load_dataset\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import os\n",
        "from pinecone import Pinecone"
      ],
      "metadata": {
        "id": "KsxutYJHOx9R"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "# Instantiate the OpenAI client\n",
        "\n",
        "index_name = 'gpt-4-langchain-docs-fast'\n",
        "dimensions = 3072\n",
        "# Configure client\n",
        "pc = Pinecone(api_key=api_key)"
      ],
      "metadata": {
        "id": "6rwMRuoNInbH"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZdWl7kR9KWJ"
      },
      "source": [
        "---\n",
        "\n",
        "🚨 _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUEJ6vDum8q"
      },
      "source": [
        "In this example, we will use a pre-embedding dataset of the LangChain docs from [python.langchain.readthedocs.com/](https://python.langchain.com/en/latest/). If you'd like to see how we perform the data preparation refer to [this notebook]().\n",
        "\n",
        "The embeddings were produced with OpenAI's `text-embedding-ada-002` model which outputs embeddings with dimension `1536`.\n",
        "\n",
        "Let's go ahead and download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "phk1tQ0U92DM"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset('langchain-python-docs-text-embedding-ada-002')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = dataset.documents.copy()[0:200]\n",
        "model=\"text-embedding-ada-002\"\n",
        "model=\"text-embedding-3-large\"\n"
      ],
      "metadata": {
        "id": "G79xD4msNpar"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate embeddings\n",
        "def generate_embeddings(text):\n",
        "\n",
        "# Assuming you have your OpenAI API key set as an environment variable\n",
        "  client = OpenAI(api_key=openai_api_key)\n",
        "  #openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "  # response = openai.Embedding.create(\n",
        "  #   input=text,\n",
        "  #     model=model\n",
        "  # )\n",
        "  response = client.embeddings.create(\n",
        "      input=text,\n",
        "      model=model\n",
        "  )\n",
        "\n",
        "  #return response['data'][0]['embedding']\n",
        "  return response\n"
      ],
      "metadata": {
        "id": "45Am_GrbOdEL"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=generate_embeddings(\"how do i make chimichangas?\")\n",
        "#print(x['data'][0]['Embedding'])\n",
        "x.data[0].embedding"
      ],
      "metadata": {
        "id": "vug8uEE9PoUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df3.values[:2])"
      ],
      "metadata": {
        "id": "dXNHhRd9Pi8T",
        "outputId": "f017cc06-b29d-4b54-917a-9feaa9f78ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['417ede5d-39be-498f-b518-f47ed4e53b90'\n",
            "  array([ 0.00594974,  0.01983248, -0.00384342, ..., -0.00060922,\n",
            "         -0.01963556, -0.04669775])                               None\n",
            "  None\n",
            "  {'chunk': 0, 'text': '.rst\\n.pdf\\nWelcome to LangChain\\n Contents \\nGetting Started\\nModules\\nUse Cases\\nReference Docs\\nEcosystem\\nAdditional Resources\\nWelcome to LangChain#\\nLangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model, but will also be:\\nData-aware: connect a language model to other sources of data\\nAgentic: allow a language model to interact with its environment\\nThe LangChain framework is designed around these principles.\\nThis is the Python specific portion of the documentation. For a purely conceptual guide to LangChain, see here. For the JavaScript documentation, see here.\\nGetting Started#\\nHow to get started using LangChain to create an Language Model application.\\nQuickstart Guide\\nConcepts and terminology.\\nConcepts and terminology\\nTutorials created by community experts and presented on YouTube.\\nTutorials\\nModules#\\nThese modules are the core abstractions which we view as the building blocks of any LLM-powered application.\\nFor each module LangChain provides standard, extendable interfaces. LangChain also provides external integrations and even end-to-end implementations for off-the-shelf use.\\nThe docs for each module contain quickstart examples, how-to guides, reference docs, and conceptual guides.\\nThe modules are (from least to most complex):\\nModels: Supported model types and integrations.\\nPrompts: Prompt management, optimization, and serialization.\\nMemory: Memory refers to state that is persisted between calls of a chain/agent.\\nIndexes: Language models become much more powerful when combined with application-specific data - this module contains interfaces and integrations for loading, querying and updating external data.\\nChains: Chains are structured sequences of calls (to an LLM or to a different utility).\\nAgents: An agent is a Chain in which an LLM, given a high-level directive and a set of tools, repeatedly decides an action, executes the action and observes the outcome until the high-level directive is complete.\\nCallbacks: Callbacks let you log and stream the intermediate steps of any chain, making it easy to observe, debug, and evaluate the internals of an application.\\nUse Cases#\\nBest practices and built-in implementations for common LangChain use cases:', 'url': 'https://python.langchain.com/en/latest/index.html'}]\n",
            " ['110f550d-110b-4378-b95e-141397fa21bc'\n",
            "  array([ 0.00940175,  0.02443608,  0.01027699, ..., -0.01255684,\n",
            "         -0.01763181, -0.02747118])                               None\n",
            "  None\n",
            "  {'chunk': 1, 'text': 'Use Cases#\\nBest practices and built-in implementations for common LangChain use cases:\\nAutonomous Agents: Autonomous agents are long-running agents that take many steps in an attempt to accomplish an objective. Examples include AutoGPT and BabyAGI.\\nAgent Simulations: Putting agents in a sandbox and observing how they interact with each other and react to events can be an effective way to evaluate their long-range reasoning and planning abilities.\\nPersonal Assistants: One of the primary LangChain use cases. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: Another common LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Language models love to chat, making this a very natural use of them.\\nQuerying Tabular Data: Recommended reading if you want to use language models to query structured data (CSVs, SQL, dataframes, etc).\\nCode Understanding: Recommended reading if you want to use language models to analyze code.\\nInteracting with APIs: Enabling language models to interact with APIs is extremely powerful. It gives them access to up-to-date information and allows them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Compressing longer documents. A type of Data-Augmented Generation.\\nEvaluation: Generative models are hard to evaluate with traditional metrics. One promising approach is to use language models themselves to do the evaluation.\\nReference Docs#\\nFull documentation on all methods, classes, installation methods, and integration setups for LangChain.\\nLangChain Installation\\nReference Documentation\\nEcosystem#\\nLangChain integrates a lot of different LLMs, systems, and products.\\nFrom the other side, many systems and products depend on LangChain.\\nIt creates a vibrant and thriving ecosystem.\\nIntegrations: Guides for how other products can be used with LangChain.\\nDependents: List of repositories that use LangChain.\\nDeployments: A collection of instructions, code snippets, and template repositories for deploying LangChain apps.\\nAdditional Resources#\\nAdditional resources we think may be useful as you develop your application!\\nLangChainHub: The LangChainHub is a place to share and explore other prompts, chains, and agents.', 'url': 'https://python.langchain.com/en/latest/index.html'}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Iterate through the dataset and embed the metadata 'text'\n",
        "for i in range(len(df.values)):\n",
        "    text_value = df.metadata[i]['text']\n",
        "    embedding = generate_embeddings(text_value)\n",
        "    df.values[i] = embedding\n",
        "\n",
        "# You can then use dataset2 as your embedded dataset.\n",
        "# For example, you could save it to a new file or upload it to a vector database.\n",
        "\n",
        "# Example: print the first 5 embeddings\n",
        "print(df.values[:5])"
      ],
      "metadata": {
        "id": "qRheFfL0OEr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Yv53PFcMfMa_",
        "outputId": "60c17774-c0f3-406c-ad03-2587a45909c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                     id  \\\n",
              "0  417ede5d-39be-498f-b518-f47ed4e53b90   \n",
              "1  110f550d-110b-4378-b95e-141397fa21bc   \n",
              "2  d5f00f02-3295-4567-b297-5e3262dc2728   \n",
              "3  0b6fe3c6-1f0e-4608-a950-43231e46b08a   \n",
              "4  39d5f15f-b973-42c0-8c9b-a2df49b627dc   \n",
              "\n",
              "                                              values  \\\n",
              "0  [0.005949743557721376, 0.01983247883617878, -0...   \n",
              "1  [0.009401749819517136, 0.02443608082830906, 0....   \n",
              "2  [-0.005517194513231516, 0.0208403542637825, 0....   \n",
              "3  [-0.006499645300209522, 0.0011573900701478124,...   \n",
              "4  [-0.005658374633640051, 0.00817849114537239, 0...   \n",
              "\n",
              "                                            metadata  \n",
              "0  {'chunk': 0, 'text': '.rst\n",
              ".pdf\n",
              "Welcome to Lan...  \n",
              "1  {'chunk': 1, 'text': 'Use Cases#\n",
              "Best practice...  \n",
              "2  {'chunk': 2, 'text': 'Gallery: A collection of...  \n",
              "3  {'chunk': 0, 'text': 'Search\n",
              "Error\n",
              "Please acti...  \n",
              "4  {'chunk': 0, 'text': '.md\n",
              ".pdf\n",
              "Dependents\n",
              "Depe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b42bf36d-2587-46a5-9879-e5e81bab5960\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>417ede5d-39be-498f-b518-f47ed4e53b90</td>\n",
              "      <td>[0.005949743557721376, 0.01983247883617878, -0...</td>\n",
              "      <td>{'chunk': 0, 'text': '.rst\n",
              ".pdf\n",
              "Welcome to Lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>110f550d-110b-4378-b95e-141397fa21bc</td>\n",
              "      <td>[0.009401749819517136, 0.02443608082830906, 0....</td>\n",
              "      <td>{'chunk': 1, 'text': 'Use Cases#\n",
              "Best practice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>d5f00f02-3295-4567-b297-5e3262dc2728</td>\n",
              "      <td>[-0.005517194513231516, 0.0208403542637825, 0....</td>\n",
              "      <td>{'chunk': 2, 'text': 'Gallery: A collection of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0b6fe3c6-1f0e-4608-a950-43231e46b08a</td>\n",
              "      <td>[-0.006499645300209522, 0.0011573900701478124,...</td>\n",
              "      <td>{'chunk': 0, 'text': 'Search\n",
              "Error\n",
              "Please acti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>39d5f15f-b973-42c0-8c9b-a2df49b627dc</td>\n",
              "      <td>[-0.005658374633640051, 0.00817849114537239, 0...</td>\n",
              "      <td>{'chunk': 0, 'text': '.md\n",
              ".pdf\n",
              "Dependents\n",
              "Depe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b42bf36d-2587-46a5-9879-e5e81bab5960')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b42bf36d-2587-46a5-9879-e5e81bab5960 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b42bf36d-2587-46a5-9879-e5e81bab5960');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7876ec3a-0670-4fb0-93d9-e6b189a72b85\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7876ec3a-0670-4fb0-93d9-e6b189a72b85')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7876ec3a-0670-4fb0-93d9-e6b189a72b85 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"110f550d-110b-4378-b95e-141397fa21bc\",\n          \"39d5f15f-b973-42c0-8c9b-a2df49b627dc\",\n          \"d5f00f02-3295-4567-b297-5e3262dc2728\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"values\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"metadata\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# We drop the sparse_values column since it is not needed in this demo\n",
        "df.drop(['sparse_values'], axis=1, inplace=True)\n",
        "# We rename the blob column to metadata\n",
        "df.drop(['metadata'], axis=1, inplace=True)\n",
        "df.rename(columns={'blob': 'metadata'}, inplace=True)\n",
        "\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avaKReiCfMa_"
      },
      "source": [
        "Let's take a look at what sort of metadata we're working with in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1OyqJxIfMa_",
        "outputId": "e0a4afcf-b37a-41b1-c988-4cebde95f688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here are some example entries in our Knowledge Base:\n",
            "\n",
            "{'chunk': 0,\n",
            " 'text': '.rst\\n'\n",
            "         '.pdf\\n'\n",
            "         'Welcome to LangChain\\n'\n",
            "         ' Contents \\n'\n",
            "         'Getting Started\\n'\n",
            "         'Modules\\n'\n",
            "         'Use Cases\\n'\n",
            "         'Reference Docs\\n'\n",
            "         'Ecosystem\\n'\n",
            "         'Additional Resources\\n'\n",
            "         'Welcome to LangChain#\\n'\n",
            "         'LangChain is a framework for developing applications powered by '\n",
            "         'language models. We believe that the most powerful and '\n",
            "         'differentiated applications will not only call out to a language '\n",
            "         'model, but will also be:\\n'\n",
            "         'Data-aware: connect a language model to other sources of data\\n'\n",
            "         'Agentic: allow a language model to interact with its environment\\n'\n",
            "         'The LangChain framework is designed around these principles.\\n'\n",
            "         'This is the Python specific portion of the documentation. For a '\n",
            "         'purely conceptual guide to LangChain, see here. For the JavaScript '\n",
            "         'documentation, see here.\\n'\n",
            "         'Getting Started#\\n'\n",
            "         'How to get started using LangChain to create an Language Model '\n",
            "         'application.\\n'\n",
            "         'Quickstart Guide\\n'\n",
            "         'Concepts and terminology.\\n'\n",
            "         'Concepts and terminology\\n'\n",
            "         'Tutorials created by community experts and presented on YouTube.\\n'\n",
            "         'Tutorials\\n'\n",
            "         'Modules#\\n'\n",
            "         'These modules are the core abstractions which we view as the '\n",
            "         'building blocks of any LLM-powered application.\\n'\n",
            "         'For each module LangChain provides standard, extendable interfaces. '\n",
            "         'LangChain also provides external integrations and even end-to-end '\n",
            "         'implementations for off-the-shelf use.\\n'\n",
            "         'The docs for each module contain quickstart examples, how-to guides, '\n",
            "         'reference docs, and conceptual guides.\\n'\n",
            "         'The modules are (from least to most complex):\\n'\n",
            "         'Models: Supported model types and integrations.\\n'\n",
            "         'Prompts: Prompt management, optimization, and serialization.\\n'\n",
            "         'Memory: Memory refers to state that is persisted between calls of a '\n",
            "         'chain/agent.\\n'\n",
            "         'Indexes: Language models become much more powerful when combined '\n",
            "         'with application-specific data - this module contains interfaces and '\n",
            "         'integrations for loading, querying and updating external data.\\n'\n",
            "         'Chains: Chains are structured sequences of calls (to an LLM or to a '\n",
            "         'different utility).\\n'\n",
            "         'Agents: An agent is a Chain in which an LLM, given a high-level '\n",
            "         'directive and a set of tools, repeatedly decides an action, executes '\n",
            "         'the action and observes the outcome until the high-level directive '\n",
            "         'is complete.\\n'\n",
            "         'Callbacks: Callbacks let you log and stream the intermediate steps '\n",
            "         'of any chain, making it easy to observe, debug, and evaluate the '\n",
            "         'internals of an application.\\n'\n",
            "         'Use Cases#\\n'\n",
            "         'Best practices and built-in implementations for common LangChain use '\n",
            "         'cases:',\n",
            " 'url': 'https://python.langchain.com/en/latest/index.html'}\n",
            "{'chunk': 1,\n",
            " 'text': 'Use Cases#\\n'\n",
            "         'Best practices and built-in implementations for common LangChain use '\n",
            "         'cases:\\n'\n",
            "         'Autonomous Agents: Autonomous agents are long-running agents that '\n",
            "         'take many steps in an attempt to accomplish an objective. Examples '\n",
            "         'include AutoGPT and BabyAGI.\\n'\n",
            "         'Agent Simulations: Putting agents in a sandbox and observing how '\n",
            "         'they interact with each other and react to events can be an '\n",
            "         'effective way to evaluate their long-range reasoning and planning '\n",
            "         'abilities.\\n'\n",
            "         'Personal Assistants: One of the primary LangChain use cases. '\n",
            "         'Personal assistants need to take actions, remember interactions, and '\n",
            "         'have knowledge about your data.\\n'\n",
            "         'Question Answering: Another common LangChain use case. Answering '\n",
            "         'questions over specific documents, only utilizing the information in '\n",
            "         'those documents to construct an answer.\\n'\n",
            "         'Chatbots: Language models love to chat, making this a very natural '\n",
            "         'use of them.\\n'\n",
            "         'Querying Tabular Data: Recommended reading if you want to use '\n",
            "         'language models to query structured data (CSVs, SQL, dataframes, '\n",
            "         'etc).\\n'\n",
            "         'Code Understanding: Recommended reading if you want to use language '\n",
            "         'models to analyze code.\\n'\n",
            "         'Interacting with APIs: Enabling language models to interact with '\n",
            "         'APIs is extremely powerful. It gives them access to up-to-date '\n",
            "         'information and allows them to take actions.\\n'\n",
            "         'Extraction: Extract structured information from text.\\n'\n",
            "         'Summarization: Compressing longer documents. A type of '\n",
            "         'Data-Augmented Generation.\\n'\n",
            "         'Evaluation: Generative models are hard to evaluate with traditional '\n",
            "         'metrics. One promising approach is to use language models themselves '\n",
            "         'to do the evaluation.\\n'\n",
            "         'Reference Docs#\\n'\n",
            "         'Full documentation on all methods, classes, installation methods, '\n",
            "         'and integration setups for LangChain.\\n'\n",
            "         'LangChain Installation\\n'\n",
            "         'Reference Documentation\\n'\n",
            "         'Ecosystem#\\n'\n",
            "         'LangChain integrates a lot of different LLMs, systems, and '\n",
            "         'products.\\n'\n",
            "         'From the other side, many systems and products depend on LangChain.\\n'\n",
            "         'It creates a vibrant and thriving ecosystem.\\n'\n",
            "         'Integrations: Guides for how other products can be used with '\n",
            "         'LangChain.\\n'\n",
            "         'Dependents: List of repositories that use LangChain.\\n'\n",
            "         'Deployments: A collection of instructions, code snippets, and '\n",
            "         'template repositories for deploying LangChain apps.\\n'\n",
            "         'Additional Resources#\\n'\n",
            "         'Additional resources we think may be useful as you develop your '\n",
            "         'application!\\n'\n",
            "         'LangChainHub: The LangChainHub is a place to share and explore other '\n",
            "         'prompts, chains, and agents.',\n",
            " 'url': 'https://python.langchain.com/en/latest/index.html'}\n",
            "{'chunk': 2,\n",
            " 'text': 'Gallery: A collection of great projects that use Langchain, compiled '\n",
            "         'by the folks at Kyrolabs. Useful for finding inspiration and example '\n",
            "         'implementations.\\n'\n",
            "         'Tracing: A guide on using tracing in LangChain to visualize the '\n",
            "         'execution of chains and agents.\\n'\n",
            "         'Model Laboratory: Experimenting with different prompts, models, and '\n",
            "         'chains is a big part of developing the best possible application. '\n",
            "         'The ModelLaboratory makes it easy to do so.\\n'\n",
            "         'Discord: Join us on our Discord to discuss all things LangChain!\\n'\n",
            "         'YouTube: A collection of the LangChain tutorials and videos.\\n'\n",
            "         'Production Support: As you move your LangChains into production, '\n",
            "         'we’d love to offer more comprehensive support. Please fill out this '\n",
            "         'form and we’ll set up a dedicated support Slack channel.\\n'\n",
            "         'next\\n'\n",
            "         'Quickstart Guide\\n'\n",
            "         ' Contents\\n'\n",
            "         '  \\n'\n",
            "         'Getting Started\\n'\n",
            "         'Modules\\n'\n",
            "         'Use Cases\\n'\n",
            "         'Reference Docs\\n'\n",
            "         'Ecosystem\\n'\n",
            "         'Additional Resources\\n'\n",
            "         'By Harrison Chase\\n'\n",
            "         '    \\n'\n",
            "         '      © Copyright 2023, Harrison Chase.\\n'\n",
            "         '      \\n'\n",
            "         '  Last updated on May 30, 2023.',\n",
            " 'url': 'https://python.langchain.com/en/latest/index.html'}\n"
          ]
        }
      ],
      "source": [
        "from pprint import pprint\n",
        "\n",
        "print(\"Here are some example entries in our Knowledge Base:\\n\")\n",
        "for r in dataset.documents.iloc[0:1].to_dict(orient=\"records\"):\n",
        "    pprint(r['metadata'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JegURaAg2PuN"
      },
      "source": [
        "Our chunks are ready so now we move onto embedding and indexing everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQUX8IzWfMbA"
      },
      "source": [
        "## Initializing the Pinecone client\n",
        "\n",
        "Now the data is ready, we can set up our index to store it.\n",
        "\n",
        "We begin by instantiating the Pinecone client. To do this we need a [free API key](https://app.pinecone.io)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "AXupo44GfMbA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if not os.environ.get(\"PINECONE_API_KEY\"):\n",
        "    from pinecone_notebooks.colab import Authenticate\n",
        "    Authenticate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pc.delete_index(name = index_name)"
      ],
      "metadata": {
        "id": "q5kbyiegHQNd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF1v9bmyfMbA"
      },
      "source": [
        "### Creating a Pinecone Index\n",
        "\n",
        "When creating the index we need to define several configuration properties.\n",
        "\n",
        "- `name` can be anything we like. The name is used as an identifier for the index when performing other operations such as `describe_index`, `delete_index`, and so on.\n",
        "- `metric` specifies the similarity metric that will be used later when you make queries to the index.\n",
        "- `dimension` should correspond to the dimension of the dense vectors produced by your embedding model. In this quick start, we are using made-up data so a small value is simplest.\n",
        "- `spec` holds a specification which tells Pinecone how you would like to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects).\n",
        "\n",
        "There are more configurations available, but this minimal set will get us started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "0ed5a6e1-c7dc-4a8c-fc17-a97af5e26fd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\n",
              "    \"name\": \"gpt-4-langchain-docs-fast\",\n",
              "    \"dimension\": 1536,\n",
              "    \"metric\": \"cosine\",\n",
              "    \"host\": \"gpt-4-langchain-docs-fast-b78vvil.svc.aped-4627-b74a.pinecone.io\",\n",
              "    \"spec\": {\n",
              "        \"serverless\": {\n",
              "            \"cloud\": \"aws\",\n",
              "            \"region\": \"us-east-1\"\n",
              "        }\n",
              "    },\n",
              "    \"status\": {\n",
              "        \"ready\": true,\n",
              "        \"state\": \"Ready\"\n",
              "    },\n",
              "    \"deletion_protection\": \"disabled\"\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "from pinecone import ServerlessSpec\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if not pc.has_index(name=index_name):\n",
        "    # if does not exist, create index\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimensions,  # dimensionality of text-embedding-ada-002\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "\n",
        "    )\n",
        "    print(\"Created index \", index_name)\n",
        "pc.describe_index(name=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9ocsVktfMbA"
      },
      "source": [
        "## Storing data in the Index\n",
        "\n",
        "First we need to instantiate an Index client that can interact with the index we just created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yMgj-3rfMbA",
        "outputId": "f39efd7c-7e9c-42a1-f47a-fe63ddcc1603"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {},\n",
              " 'total_vector_count': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Instantiate an Index client\n",
        "index = pc.Index(name=index_name)\n",
        "\n",
        "# View index stats for the new index\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSTzN2rPa2o"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "9f0662c5dc0a49a0a7ae2217764a3d18",
            "06a71c3ed2f14e13a448933278128cbc",
            "14b234fb85214d4598318c2dbf7bba85",
            "709ae8c8fbeb45cc9300610b1c72f2c1",
            "84b9a61ad5d64912b87d5270d5d14abc",
            "4d3e65c0a3e441a7b9204f02c60aa54d",
            "9d5b9e76149e4dfeb0f1948ccc0e5026",
            "34cd99b68d76446bb4e0d955c90c6b66",
            "b671f14e09604efc9625019eab702687",
            "e662b6b1d5f240b2a4a49a78c66ebe55",
            "b3ed51feaf28406b85a46d5dad675c48"
          ]
        },
        "id": "iZbFbulAPeop",
        "outputId": "0dd2b192-f90d-4e55-a60d-44854f72c96c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sending upsert requests:   0%|          | 0/6952 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f0662c5dc0a49a0a7ae2217764a3d18"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'upserted_count': 6952}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "index.upsert_from_dataframe(\n",
        "    df=df,\n",
        "    batch_size=100\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttJOrEtQIF9"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumVmMRlQQ7w"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLRODeL-QTJ9"
      },
      "source": [
        "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dhDTQnqp-yyx"
      },
      "outputs": [],
      "source": [
        "def create_embedding(query):\n",
        "    from openai import OpenAI\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Get OpenAI api key from platform.openai.com\n",
        "    #openai_api_key = os.getenv('OPENAI_API_KEY') or 'sk-...'\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    # Instantiate the OpenAI client\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "    model=\"text-embedding-ada-002\"\n",
        "    model=\"text-embedding-3-large\"\n",
        "\n",
        "    # Create an embedding\n",
        "    res = client.embeddings.create(\n",
        "      model=model,\n",
        "      input=[query],\n",
        "    )\n",
        "    return res.data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FMUPdX9cQQYC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "outputId": "0e30d390-5129-48a2-a208-43e201148729"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "PineconeApiException",
          "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Tue, 25 Mar 2025 20:59:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '104', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '55', 'x-pinecone-request-id': '5828896700232144065', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Vector dimension 3072 does not match the dimension of the index 1536\",\"details\":[]}\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPineconeApiException\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-acd859cc677a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# get relevant contexts (including the questions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minclude_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/utils/error_handling.py\u001b[0m in \u001b[0;36minner_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMaxRetryError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mProtocolError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \"\"\"\n\u001b[1;32m    440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         response = self._query(\n\u001b[0m\u001b[1;32m    442\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/data/index.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, top_k, vector, id, namespace, filter, include_values, include_metadata, sparse_vector, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m         )\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         response = self._vector_api.query(\n\u001b[0m\u001b[1;32m    500\u001b[0m             QueryRequest(\n\u001b[1;32m    501\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0margs_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \"\"\"\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/data/api/data_plane_api.py\u001b[0m in \u001b[0;36m__query\u001b[0;34m(self, query_request, **kwargs)\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_host_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"query_request\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         self.query = _Endpoint(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_with_http_info\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         return self.api_client.call_api(\n\u001b[0m\u001b[1;32m    880\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"endpoint_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"http_method\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, async_threadpool_executor, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0masync_req\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             return self.__call_api(\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mresource_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPineconeApiException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout, _host, _check_type)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;31m# perform request and return response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             response_data = self.request(\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    516\u001b[0m             )\n\u001b[1;32m    517\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             return self.rest_client.POST(\n\u001b[0m\u001b[1;32m    519\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mquery_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     ):\n\u001b[0;32m--> 345\u001b[0;31m         return self.request(\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pinecone/core/openapi/shared/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mServiceException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPineconeApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPineconeApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Date': 'Tue, 25 Mar 2025 20:59:33 GMT', 'Content-Type': 'application/json', 'Content-Length': '104', 'Connection': 'keep-alive', 'x-pinecone-request-latency-ms': '55', 'x-pinecone-request-id': '5828896700232144065', 'x-envoy-upstream-service-time': '2', 'server': 'envoy'})\nHTTP response body: {\"code\":3,\"message\":\"Vector dimension 3072 does not match the dimension of the index 1536\",\"details\":[]}\n"
          ]
        }
      ],
      "source": [
        "query = \"how do I use the LLMChain in LangChain?\"\n",
        "\n",
        "query =\"are chimichangas better than enchiladas?\"\n",
        "# retrieve from Pinecone\n",
        "xq = create_embedding(query)\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(vector=xq, top_k=5, include_metadata=True)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBSiDLIUADZ"
      },
      "source": [
        "With retrieval complete, we move on to feeding these into GPT-4 to produce answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzS4-6-UXgX"
      },
      "source": [
        "## Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPC1jQaKUcy0"
      },
      "source": [
        "GPT-4 is currently accessed via the `ChatCompletions` endpoint of OpenAI.\n",
        "\n",
        "To get a richer response from the LLM that includes context from our knowledge base, we need to retrieve context relevant to the query and then include it into the chat completion prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "def retrieval_augmented_prompt(query):\n",
        "    context_limit = 3750\n",
        "    xq = create_embedding(query)\n",
        "\n",
        "    # Get relevant contexts\n",
        "    query_results = index.query(vector=xq, top_k=3, include_metadata=True)\n",
        "    contexts = [\n",
        "        x.metadata['text'] for x in query_results.matches\n",
        "    ]\n",
        "\n",
        "    # Build our prompt with the retrieved contexts included\n",
        "    prompt_start = (\n",
        "        \"Answer the question based on the context below.\\n\\n\"+\n",
        "        \"Context:\\n\"\n",
        "    )\n",
        "    prompt_end = (\n",
        "        f\"\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    )\n",
        "    context_separator = \"\\n\\n---\\n\\n\"\n",
        "\n",
        "    # Join contexts and trim to fit within limit\n",
        "    combined_contexts = []\n",
        "    total_length = 0\n",
        "\n",
        "    for context in contexts:\n",
        "        new_length = total_length + len(context) + len(context_separator)\n",
        "        if new_length >= context_limit:\n",
        "            break\n",
        "        combined_contexts.append(context)\n",
        "        total_length = new_length\n",
        "\n",
        "    return prompt_start + context_separator.join(combined_contexts) + prompt_end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "01a7e644-2123-4558-a847-99c3a8f374b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer the question based on the context below.\n",
            "\n",
            "Context:\n",
            "for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n\\nResources (high-level explanation of core concepts)\\n\\nð\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\nð\\x9f“\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\nð\\x9f”\\x97 Chains:\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\nð\\x9f“\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\nð\\x9f¤\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain\n",
            "\n",
            "---\n",
            "\n",
            "an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously\n",
            "\n",
            "Question: how do I use the LLMChain in LangChain?\n",
            "Answer:\n"
          ]
        }
      ],
      "source": [
        "prompt = retrieval_augmented_prompt(query)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihH_GMiV5_p"
      },
      "source": [
        "Now we ask the question of the LLM using chat completion:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "def chat_completion(prompt):\n",
        "    from openai import OpenAI\n",
        "    from google.colab import userdata\n",
        "    # Get OpenAI api key from platform.openai.com\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "\n",
        "    # Instantiate the OpenAI client\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Instructions\n",
        "    sys_prompt = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "    user questions based on the information provided by the user above\n",
        "    each question. If the information can not be found in the information\n",
        "    provided by the user you truthfully say \"I don't know\".\n",
        "    \"\"\"\n",
        "\n",
        "    res = client.chat.completions.create(\n",
        "        model='gpt-4o-mini-2024-07-18',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": sys_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    return res.choices[0].message.content.strip()\n",
        "\n",
        "def rag(query):\n",
        "    prompt = retrieval_augmented_prompt(query)\n",
        "    return chat_completion(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zYg0dqlsfMbB",
        "outputId": "b6066807-f306-4523-f2c8-9c52db7db865"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't know.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "question=\"how do i make chimichangas?\"\n",
        "answer = rag(question)\n",
        "answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS1yJhOWpiJ"
      },
      "source": [
        "To display this response nicely, we will display it in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "RDo2qeMHWto1",
        "outputId": "820d9d87-69a2-44e2-915d-3b28496dbbe6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- LangChain is an intuitive framework for developing applications driven by language models, such as OpenAI or Hugging Face.\n- It is a Python library that provides out-of-the-box support for building NLP applications using LLMs.\n- LangChain offers a standard interface for creating chains, enabling sequences of calls that go beyond a single LLM call.\n- The framework simplifies the process of building advanced language model applications.\n- It includes various modules to assist with different aspects of language model application development.\n- LangChain is designed to help with six main areas: LLMs and Prompts, Chains, Data Augmented Generation, and Agents.\n- It supports prompt management, prompt optimization, and common utilities for working with LLMs.\n- LangChain facilitates integrations with other tools and provides end-to-end chains for common applications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(answer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-a8MHg0eYQ"
      },
      "source": [
        "Let's compare this to a non-augmented query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "98bbd7fd-5794-43c8-f717-bfffbe806c3a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I don't know."
          },
          "metadata": {}
        }
      ],
      "source": [
        "def non_augmented_prompt(query):\n",
        "    return f\"\"\"\n",
        "Question: {query}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "answer2 = chat_completion(non_augmented_prompt(question))\n",
        "\n",
        "display(Markdown(answer2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSsA-dW0m_P"
      },
      "source": [
        "If we drop the `\"I don't know\"` part of the `sys_prompt`, the LLM will try to pull an answer out of things it already knows. These may or may not be correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "dd92b128-4d6d-4a24-f471-3c7e088a8889"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "- **Definition**: LangChain is a framework designed for developing applications powered by language models.\n\n- **Modular Components**: It consists of various modules that can be combined to create complex applications, including:\n  - **Prompt Templates**: Tools for creating and managing prompts for language models.\n  - **Chains**: Sequences of calls to language models or other tools, allowing for multi-step workflows.\n  - **Agents**: Components that can make decisions based on user input and dynamically choose actions.\n\n- **Integration**: LangChain supports integration with various language models, including OpenAI's GPT, Hugging Face models, and others.\n\n- **Data Handling**: It provides utilities for managing and processing data, including document loaders and vector stores for efficient retrieval.\n\n- **Use Cases**: Common applications include chatbots, question-answering systems, content generation, and more.\n\n- **Extensibility**: Developers can extend LangChain with custom components to fit specific needs.\n\n- **Community and Resources**: It has an active community and offers extensive documentation, tutorials, and examples to help users get started.\n\n- **Deployment**: LangChain can be used in various environments, from local development to cloud-based applications."
          },
          "metadata": {}
        }
      ],
      "source": [
        "def hallucinating_chat_completion(prompt):\n",
        "    from openai import OpenAI\n",
        "\n",
        "    from google.colab import userdata\n",
        "    # Get OpenAI api key from platform.openai.com\n",
        "    openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    # Instantiate the OpenAI client\n",
        "    client = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "    # Instructions\n",
        "    sys_prompt = f\"\"\"You are helpful Q&A bot.\"\"\"\n",
        "\n",
        "    res = client.chat.completions.create(\n",
        "        model='gpt-4o-mini-2024-07-18',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": sys_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    return res.choices[0].message.content.strip()\n",
        "\n",
        "answer3 = hallucinating_chat_completion(non_augmented_prompt(question))\n",
        "display(Markdown(answer3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcGon5672lBb"
      },
      "source": [
        "Then we see something even worse than `\"I don't know\"` — hallucinations. Clearly augmenting our queries with additional context can make a huge difference to the performance of our system and ensure that trusted information is given priority when composing a response.\n",
        "\n",
        "Great, we've seen how to augment GPT-4 with semantic search to allow us to answer LangChain specific queries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueUJ6ntVfMbB"
      },
      "source": [
        "## Demo cleanup\n",
        "\n",
        "Once you're finished, we delete the index to save resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah_vfEHV2khx"
      },
      "outputs": [],
      "source": [
        "pc.delete_index(name=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUMlO8M2h4Y"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9f0662c5dc0a49a0a7ae2217764a3d18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_06a71c3ed2f14e13a448933278128cbc",
              "IPY_MODEL_14b234fb85214d4598318c2dbf7bba85",
              "IPY_MODEL_709ae8c8fbeb45cc9300610b1c72f2c1"
            ],
            "layout": "IPY_MODEL_84b9a61ad5d64912b87d5270d5d14abc"
          }
        },
        "06a71c3ed2f14e13a448933278128cbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d3e65c0a3e441a7b9204f02c60aa54d",
            "placeholder": "​",
            "style": "IPY_MODEL_9d5b9e76149e4dfeb0f1948ccc0e5026",
            "value": "sending upsert requests: 100%"
          }
        },
        "14b234fb85214d4598318c2dbf7bba85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34cd99b68d76446bb4e0d955c90c6b66",
            "max": 6952,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b671f14e09604efc9625019eab702687",
            "value": 6952
          }
        },
        "709ae8c8fbeb45cc9300610b1c72f2c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e662b6b1d5f240b2a4a49a78c66ebe55",
            "placeholder": "​",
            "style": "IPY_MODEL_b3ed51feaf28406b85a46d5dad675c48",
            "value": " 6952/6952 [01:22&lt;00:00, 107.01it/s]"
          }
        },
        "84b9a61ad5d64912b87d5270d5d14abc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d3e65c0a3e441a7b9204f02c60aa54d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d5b9e76149e4dfeb0f1948ccc0e5026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34cd99b68d76446bb4e0d955c90c6b66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b671f14e09604efc9625019eab702687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e662b6b1d5f240b2a4a49a78c66ebe55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3ed51feaf28406b85a46d5dad675c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}